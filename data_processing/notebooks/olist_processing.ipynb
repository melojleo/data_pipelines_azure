{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d080761a-dc1a-4d4f-a282-76faf696633c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Load, Transform, Persist Pipeline\n",
    "\n",
    "#1-mount the data lakes\n",
    "#2-loads csvs from landing data lake\n",
    "#3-convert csvs to parquet and move then to processing data lake\n",
    "#4-create sql database\n",
    "#5-create tables based on parquet format files\n",
    "#6-specific analysis wil be moved to curated data lake and then loaded into sql tables\n",
    "#7-powerbi application reads directly from sql tables at databricks rest api service\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c69b7bfc-0b07-4c96-ad27-8157f8f1a579",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Mounting Data lakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b77f75b7-53ca-405f-b28a-d09443a0d1eb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Mounting Data Lakes\n",
    "# Unmount the existing mount if it exists (to avoid conflicts)\n",
    "#dbutils.fs.unmount(f\"/mnt/{bucket_name1}\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "169567fb-b31b-4012-b847-571137446750",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define Azure storage and authentication details\n",
    "client_id = \"ee65398e-3e66-4e98-b8fa-348798024231\"\n",
    "scope_config = \"olist_scope1\"\n",
    "key_vault = \"olist-secret1\"\n",
    "directory_id = \"a23e0519-184a-4922-b08d-96f35c444623\"\n",
    "url_storage_account = \"oliststorageaccount\"\n",
    "bucket_name1=\"landing\"\n",
    "bucket_name2=\"processing\"\n",
    "bucket_name3=\"curated\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca4db3c1-1e38-43eb-a3a4-a0fce5e0549a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Configuration using variables for mounting the data lakes\n",
    "configs = {\n",
    "    \"fs.azure.account.auth.type\": \"OAuth\",\n",
    "    \"fs.azure.account.oauth.provider.type\": \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\",\n",
    "    \"fs.azure.account.oauth2.client.id\": client_id,\n",
    "    \"fs.azure.account.oauth2.client.secret\": dbutils.secrets.get(scope=scope_config, key=key_vault),\n",
    "    \"fs.azure.account.oauth2.client.endpoint\": f\"https://login.microsoftonline.com/{directory_id}/oauth2/token\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9446071-66a2-4244-aff3-64dc65db0478",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Mount the landing data lake\n",
    "\"\"\"\n",
    "# Montando o ponto de montagem\n",
    "dbutils.fs.mount(\n",
    "    source=f\"abfss://{bucket_name1}@{url_storage_account}.dfs.core.windows.net/\",\n",
    "    mount_point=f\"/mnt/{bucket_name1}\",\n",
    "    extra_configs=configs\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4fcf75a-9a48-4f5e-9bc6-cb8648569f52",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# List files in the landing data lake to verify the mount\n",
    "dbutils.fs.ls(f\"/mnt/{bucket_name1}/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ffd11874-2917-461f-a1a4-a6aabe99cf38",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Unmount the existing mount if it exists (to avoid conflicts)\n",
    "\"\"\"\n",
    "#Example: error device already monted\n",
    "dbutils.fs.unmount(f\"/mnt/{bucket_name2}\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "946de2d7-05ae-4a1c-b1c1-6821691d42e4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Mount the processing data lake\n",
    "\"\"\"\n",
    "# Montando o ponto de montagem\n",
    "dbutils.fs.mount(\n",
    "    source=f\"abfss://{bucket_name2}@{url_storage_account}.dfs.core.windows.net/\",\n",
    "    mount_point=f\"/mnt/{bucket_name2}\",\n",
    "    extra_configs=configs\n",
    ")\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1065fa24-d025-49cc-a8de-e408a99ea496",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# List files in the processing data lake to verify the mount\n",
    "\"\"\"\n",
    "dbutils.fs.ls(f\"/mnt/{bucket_name2}\")\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c62cd63-a862-449d-ba58-57199fb4d7af",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Unmount the existing mount if it exists (to avoid conflicts)\n",
    "\"\"\"\n",
    "dbutils.fs.unmount(f\"/mnt/{bucket_name3}\")\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ed31047-15e2-459b-9f69-70d7458bc98c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Mount the curated data lake\n",
    "\"\"\"\n",
    "dbutils.fs.mount(\n",
    "  source = f\"abfss://{bucket_name3}@{url_storage_account}.dfs.core.windows.net/\",\n",
    "  mount_point = f\"/mnt/{bucket_name3}\",\n",
    "  extra_configs = configs)\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc4b415e-182d-43b9-a7e4-4f53db9e932e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# List files in the curated data lake to verify the mount\n",
    "dbutils.fs.ls(f\"/mnt/{bucket_name3}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aaafb5cb-4ac5-49f6-a31b-466e919bf1c9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Readings CSVs in Landing Data Lake to DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a322e8b4-7205-4786-a67e-b810aaf80873",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Function to read data from CSV and return a DataFrame\n",
    "def read_data(format, inferSchema, header, delimiter, path_to_load):\n",
    "    return spark.read.format(format) \\\n",
    "        .option(\"inferSchema\", inferSchema) \\\n",
    "        .option(\"header\", header) \\\n",
    "        .option(\"delimiter\", delimiter) \\\n",
    "        .load(path_to_load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22410f12-4136-436c-8b80-8881b99274a7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# List of files in the landing data lake\n",
    "df_csv_list = dbutils.fs.ls(f\"/mnt/{bucket_name1}/\")\n",
    "\n",
    "# Reading parameters\n",
    "format = \"csv\"\n",
    "inferSchema = \"true\"\n",
    "header = \"true\"\n",
    "delimiter = \",\"\n",
    "\n",
    "# Dictionary to store dataframes\n",
    "dataframes = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cbfa1f6e-99e0-463c-a109-2260d5e0e260",
     "showTitle": false,
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Iterating over the files and storing them in dataframes\n",
    "for file_info in df_csv_list:\n",
    "    file_path = file_info.path\n",
    "    file_name = file_path.split('/')[-1]  \n",
    "    dataframe_name = \"df_\" + file_name.split('_', 1)[1].replace('_dataset.csv', '').replace('.csv', '')\n",
    "    \n",
    "    print(f\"Readed the path: {file_path} and created the dataframe: {dataframe_name}\")\n",
    "    \n",
    "    # Create the dataframe by calling the read_data function with provided parameters\n",
    "    dataframe = read_data(format, inferSchema, header, delimiter, file_path)\n",
    "    \n",
    "    # Store the dataframe in the dictionary with the key being the dataframe name\n",
    "    dataframes[dataframe_name] = dataframe\n",
    "    \n",
    "    print(f\"Dataframe {dataframe_name}:\")\n",
    "    display(dataframes[dataframe_name])\n",
    "    print(f\"{dataframe_name} schema:\")\n",
    "    dataframes[dataframe_name].printSchema()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51627812-96ce-43e9-a9b8-11cc7d9de2e3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Display a specific dataframe to verify\n",
    "dataframes['df_customers'].show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d38fd10b-2375-432b-9205-23fa4fd06dc0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Create SQL Temp Views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b444619b-bd6d-49a8-9c02-101c1e227e02",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Define the database name\n",
    "database_name = 'customers_db'\n",
    "\n",
    "# List all tables in the database\n",
    "tables = spark.sql(f\"SHOW TABLES IN {database_name}\")\n",
    "\n",
    "# Iterate over the tables and drop each one to start fresh\n",
    "for row in tables.collect():\n",
    "    table_name = row.tableName\n",
    "    drop_table_sql = f\"DROP TABLE IF EXISTS {database_name}.{table_name}\"\n",
    "    spark.sql(drop_table_sql)\n",
    "    print(f\"Dropped table: {database_name}.{table_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c6aeef0-52b6-4bae-85ff-821bc00d480b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create views from dataframes\n",
    "items_list = list(dataframes.items())\n",
    "for name, df in items_list:\n",
    "    view_name = name.replace('df_', '')\n",
    "    print(f\"Creating view for DataFrame: {name}\")\n",
    "    view_name = f\"view_{view_name}\"  \n",
    "    dataframes[name].createOrReplaceTempView(view_name)\n",
    "    print(f\"View created: {view_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7f5a2fa-9558-432e-826d-c79289734cb6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# List all temporary views\n",
    "temp_views = spark.catalog.listTables()\n",
    "for view in temp_views:\n",
    "    print(view.name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ffe5bbf-7fd6-4448-a461-bee358337e8a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Query a specific view to verify\n",
    "%sql\n",
    "SELECT *\n",
    "FROM view_customers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03ba92ba-7f21-420f-a7f2-1e3bb4b7d600",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Create SQL Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "889b6e15-86cc-4ce1-b7ba-7ea17a07c278",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create SQL Database\n",
    "%sql\n",
    "CREATE DATABASE IF NOT EXISTS customers_db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37e66a5d-d5d9-4c40-9914-17c09e856621",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Create SQL Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b601dad-7eb6-4f32-91a9-7140be6b97e6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# List of files in landing data lake\n",
    "df_csv_list = dbutils.fs.ls(f\"/mnt/{bucket_name1}/\")\n",
    "\n",
    "# Iterate over each CSV file\n",
    "for file_info in df_csv_list:\n",
    "    file_path = file_info.path\n",
    "    file_name = file_path.split('/')[-1]  # Obtém o nome do arquivo\n",
    "    table_name = file_name.split('_', 1)[1].replace('_dataset.csv', '').replace('.csv', '')\n",
    "\n",
    "    # Create table in Spark SQL\n",
    "    create_table_sql = f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS customers_db.{table_name}\n",
    "    USING CSV\n",
    "    OPTIONS (\n",
    "        path '{file_path}',\n",
    "        header 'true',\n",
    "        inferSchema 'true'\n",
    "    )\n",
    "    \"\"\"\n",
    "    \n",
    "    # Execute the SQL command to create the table\n",
    "    spark.sql(create_table_sql)\n",
    "    print(f\"Table created for {file_name} as {table_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5f50991-231e-47c6-9561-db4e0b846fde",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Verify if the tables are created\n",
    "spark.sql(\"SHOW TABLES IN customers_db\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea63015e-755b-4710-8251-070d8a783498",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Query the table to view its content\n",
    "%sql\n",
    "SELECT COUNT(*) FROM customers_db.customers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e2e4be0-9bde-4b98-8266-f996683a5e10",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Query the table to view its content\n",
    "SELECT *\n",
    "FROM customers_db.customers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10f024e7-824a-432b-8b25-c1268d8d2485",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Describe the table to view its schema\n",
    "%sql\n",
    "DESCRIBE customers_db.customers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af30e7a7-da4c-4af4-b423-631997efcf9d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Filtering the DataSet\n",
    "df_customers_SQL = spark.table('customers_db.customers')\n",
    "display(df_customers_SQL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea82f10e-07d7-48ad-808d-09b8499c059e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Filtering the DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "001f4c64-c982-49cb-8096-0a5b042eb5a2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_customers_SQL.select('customer_state').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a88d2315-3fb5-49ea-8f18-ed814291a303",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "df_customers_SQL = df_customers_SQL.filter(col(\"customer_state\") == \"RJ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2d513eb-543c-4f3c-9db7-cd64840b431b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_customers_SQL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "525c6448-8032-4060-b9ef-700e79154d92",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Write Full Parquet Datasets to Processing Data lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "115d6731-84bd-4a49-b624-8763200bd3d1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "items_list = list(dataframes.items())\n",
    "\n",
    "for name, df in items_list:\n",
    "    print(f\"Writing dataset {name} in processing zone\")\n",
    "    name_file = name.replace('df_','')\n",
    "    dataframes[name].write.mode(\"overwrite\").parquet(f\"/mnt/processing/{name_file}.parquet\")\n",
    "    print(f\"File processed: {name_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e156f3ab-e948-47b0-80a8-38dc17f7bea5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Write Filtered Parquet to Processing Data Lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "adbc7e8b-b4c1-4eb8-b6f6-452e89023c52",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_customers_SQL.write.mode(\"overwrite\").parquet(\"/mnt/processing/customers_RJ.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b67c791-56fc-4946-ab69-43059fcebc84",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_customers_parq = spark.read.parquet(\"/mnt/processing/customers_RJ.parquet\")\n",
    "display(df_customers_parq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0a341ca-deb0-440d-bb33-c653b55d4dfc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_customers_parq.createOrReplaceTempView(\"CustomersParquetTable\")\n",
    "custparkSQL = spark.sql(\"select * from CustomersParquetTable where customer_state = 'RJ'\")\n",
    "display(custparkSQL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dfbadc2f-8c79-4c58-862c-1c0415c2b3d3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Create SQL Tables based on Parquet files at Processing Data Lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4431def1-4078-479e-a2f1-67721cd312b9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "file_info.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "484b67f6-fec3-41bd-9897-44b6ba3a65b6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# List of files in processing data lake\n",
    "df_csv_list = dbutils.fs.ls(f\"/mnt/{bucket_name2}/\")\n",
    "\n",
    "# Iterate over each Parquet file\n",
    "for file_info in df_csv_list:\n",
    "    file_path = file_info.path\n",
    "    file_name = file_info.name  # Obtém o nome do arquivo\n",
    "    table_name = file_name.replace('.parquet/', '_pqt')\n",
    "\n",
    "    # Create table in Spark SQL\n",
    "    create_table_sql = f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS customers_db.{table_name}\n",
    "    USING PARQUET\n",
    "    OPTIONS (\n",
    "        path '{file_path}',\n",
    "        header 'true',\n",
    "        inferSchema 'true'\n",
    "    )\n",
    "    \"\"\"\n",
    "\n",
    "    print(create_table_sql)\n",
    "    \n",
    "    # Execute the SQL command to create the table\n",
    "    spark.sql(create_table_sql)\n",
    "    print(f\"Table created for {file_name} as {table_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7025450c-0893-4604-b81e-7f5e827c1435",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "REFRESH TABLE customers_db.customers_RJ_pqt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1cd0473-7f67-40ab-8f2c-dd545ee1ffbb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * from customers_db.customers_RJ_pqt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a2fafeb-b8be-4aa8-823b-1bf55b273db2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_customers_parq = spark.read.parquet(\"/mnt/processing/customers_RJ.parquet\")\n",
    "df_customers_parq.createOrReplaceTempView(\"CustomersParquetTableByState\")\n",
    "df_customers_by_state_parq = spark.sql(\"select * from CustomersParquetTableByState where customer_state='RJ'\")\n",
    "display(df_customers_by_state_parq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "841bfca8-d964-4fc4-a353-724a1cffe971",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_customers_parq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d70559a7-6108-437f-a915-9747aad1adc0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Write processed CSVs to Curated Data Lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29a1df2c-6006-4215-8f9c-6a3d4157c67c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_customers_parq.write.option(\"header\",True).option(\"delimiter\",\",\").mode(\"overwrite\").csv(\"/mnt/curated/customers_RJ.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9778f072-0bee-4f10-8629-d98ce901342f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Test Reading CSV file located at Curated Data Lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d6540b4-e595-49b1-a2dc-175bc2fa6ffe",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#read in the data to dataframe df\n",
    "df_RJ = spark.read.format(\"csv\").option(\"inferSchema\", \"true\").option(\"header\",\"true\").option(\"delimiter\",\",\").load(\"/mnt/curated/customers_RJ.csv\")\n",
    " \n",
    "#display the dataframe\n",
    "display(df_RJ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3600d3fd-8b7b-43f7-b64d-926bf0e08d3e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Filtered Curated CSV\n",
    "CREATE TABLE IF NOT EXISTS customers_db.customers_RJ_csv \n",
    "USING CSV\n",
    "LOCATION '/mnt/curated/customers_RJ.csv'\n",
    "OPTIONS (header \"true\", inferSchema \"true\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "039e17f8-756d-4c6f-93f7-39ff22932865",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "REFRESH TABLE customers_db.customers_RJ_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69367b09-9a88-48a4-947d-6211fb17dddf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "Select * from customers_db.customers_RJ_csv "
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4421327225283907,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "olist_processing",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
